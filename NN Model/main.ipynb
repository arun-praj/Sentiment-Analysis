{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "import pickle\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from keras.models import Model\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.layers import Input, Dropout,BatchNormalization,Conv1D,GlobalAveragePooling1D,Activation, Embedding\r\n",
    "\r\n",
    "training_set=pd.read_csv('../dataset/labeledTrainData.tsv',sep='\\t')\r\n",
    "training_set.head(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "# train_test_split\r\n",
    "y_train=training_set['sentiment'].values\r\n",
    "x_train=training_set['review'].values\r\n",
    "train_data,test_data,train_labels,test_labels=train_test_split(x_train,y_train,shuffle=True,test_size=0.2,random_state=42,stratify=y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "y_train = [[1,0] if y==0 else [0,1] for y in train_labels ]\r\n",
    "y_test = [[1,0] if y==0 else [0,1] for y in test_labels ]\r\n",
    "\r\n",
    "y_train = np.array(y_train)\r\n",
    "y_test = np.array(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "max_features = 8192\r\n",
    "maxlen = 128\r\n",
    "embed_size = 64"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\r\n",
    "tokenizer.fit_on_texts(train_data)\r\n",
    "\r\n",
    "# save tokenizer for testing\r\n",
    "import pickle\r\n",
    "with open('tokenizer.sav','wb') as handle:\r\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "\r\n",
    "token_train = tokenizer.texts_to_sequences(train_data)\r\n",
    "token_test = tokenizer.texts_to_sequences(test_data)\r\n",
    "\r\n",
    "x_train = pad_sequences(token_train, maxlen=maxlen, padding='post')\r\n",
    "x_test = pad_sequences(token_test, maxlen=maxlen, padding='post')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "input = Input(shape=(maxlen,))\r\n",
    "net = Embedding(max_features, embed_size)(input)\r\n",
    "net = Dropout(0.2)(net)\r\n",
    "net = BatchNormalization()(net)\r\n",
    "net = Conv1D(32, 7, padding='same', activation='relu')(net)\r\n",
    "net = BatchNormalization()(net)\r\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\r\n",
    "net = BatchNormalization()(net)\r\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\r\n",
    "net = BatchNormalization()(net)\r\n",
    "net = Conv1D(32, 3, padding='same', activation='relu')(net)\r\n",
    "net1 = BatchNormalization()(net)\r\n",
    "net = Conv1D(2, 1)(net)\r\n",
    "net = GlobalAveragePooling1D()(net)\r\n",
    "output = Activation('softmax')(net)\r\n",
    "model = Model(inputs = input, outputs = output)\r\n",
    "\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "model.fit(x_train, y_train, batch_size=2048, epochs=5, validation_split=0.1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 29s 3s/step - loss: 0.6777 - acc: 0.5854 - val_loss: 0.6933 - val_acc: 0.4925\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 30s 3s/step - loss: 0.5198 - acc: 0.7649 - val_loss: 0.6775 - val_acc: 0.6065\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 32s 4s/step - loss: 0.3394 - acc: 0.8561 - val_loss: 0.6354 - val_acc: 0.8350\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.2433 - acc: 0.8988 - val_loss: 0.6686 - val_acc: 0.4950\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 31s 3s/step - loss: 0.1874 - acc: 0.9266 - val_loss: 0.6928 - val_acc: 0.4930\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab0d2ca520>"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "model.evaluate(x_test, y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "157/157 [==============================] - 2s 14ms/step - loss: 0.6868 - acc: 0.5004\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.6867843866348267, 0.5004000067710876]"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "test_input = tokenizer.texts_to_sequences('hello')\r\n",
    "print(test_input)\r\n",
    "\r\n",
    "test_input = pad_sequences(test_input, maxlen=maxlen, padding='post')\r\n",
    "# print(test_input)\r\n",
    "\r\n",
    "model.predict(test_input)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1902], [945], [2006], [2006], [1715]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.6360091 , 0.36399093],\n",
       "       [0.6359386 , 0.36406142],\n",
       "       [0.6382874 , 0.36171263],\n",
       "       [0.6382874 , 0.36171263],\n",
       "       [0.6367879 , 0.3632121 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "a456efd1d2e2bfa10bfdad488db5626e5f8bd233a0f11ae70ce0e7717a6a7d8d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}